global:
  extraValueFiles: []
  pattern: common
  secretLoader:
    disabled: false
  secretStore:
    backend: "vault"
  targetRevision: main
  options:
    useCSV: True
    # -- This defines the global syncpolicy. If set to "Manual", no syncPolicy object will be set, if set to "Automatic" syncPolicy will be set to {automated: {}, retry: { limit: global.options.applicationRetryLimit }}, if set to an object it will be passed directly to the syncPolicy field of the application. Each application can override this
    syncPolicy: Automatic
    installPlanApproval: Automatic
    applicationRetryLimit: 20

# Note that sometimes changing helm values might require a hard refresh (https://github.com/helm/helm/issues/3486)
clusterGroup:
  name: example
  # DEPRECATED: isHubCluster is deprecated. Use global.localClusterDomain and global.hubClusterDomain instead.
  # When both domain values are set, hub cluster detection will be based on whether they are equal.
  # This field is kept for backwards compatibility when domain values are not set.
  # isHubCluster: true
  targetCluster: in-cluster
  sharedValueFiles: []

#  scheduler:
#    mastersSchedulable: true
#    defaultNodeSelector: type=user-node,region=east
#    profile: HighNodeUtilization

  argoCD:
    initContainers: []
    volumes: []
    configManagementPlugins: []
    # resource tracking can be set to annotation, label, or annotation+label
    resourceTrackingMethod: label
    resourceHealthChecks:
      # Adding health checks to argocd to prevent pvc resources
      # that aren't bound state from blocking deployments
      # https://www.github.com/argoproj/argo-cd/issues/12840 seems to be related
      - kind: PersistentVolumeClaim
        check: |
          hs = {}
          if obj.status ~= nil then
            if obj.status.phase ~= nil then
              if obj.status.phase == "Pending" then
                hs.status = "Healthy"
                hs.message = obj.status.phase
                return hs
              elseif obj.status.phase == "Bound" then
                hs.status = "Healthy"
                hs.message = obj.status.phase
                return hs
              end
            end
          end
          hs.status = "Progressing"
          hs.message = "Waiting for PVC"
          return hs

      # Drop once upstream argo cd handles the stopped field correctly
      # As of 20251001 there is not a pr/issue in argo yet
      - kind: InferenceService
        group: serving.kserve.io
        check: |
          local health_status = {}

          health_status.status = "Progressing"
          health_status.message = "Waiting for InferenceService to report status..."

          if obj.status ~= nil then

            local progressing = false
            local degraded = false
            local status_false = 0
            local status_unknown = 0
            local msg = ""

            if obj.status.modelStatus ~= nil then
              if obj.status.modelStatus.transitionStatus ~= "UpToDate" then
                if obj.status.modelStatus.transitionStatus == "InProgress" then
                  progressing = true
                else
                  degraded = true
                end
                msg = msg .. "0: transitionStatus | " .. obj.status.modelStatus.transitionStatus
              end
            end

            if obj.status.conditions ~= nil then
              for i, condition in pairs(obj.status.conditions) do

                -- A condition is healthy if its status is True.
                -- However, for the 'Stopped' condition, a 'False' status is the healthy state.
                local is_healthy_condition = (condition.status == "True")
                if condition.type == "Stopped" then
                  is_healthy_condition = (condition.status == "False")
                end

                if not is_healthy_condition then
                  -- This condition represents a problem, so update counters and the message.
                  if condition.status == "Unknown" then
                    status_unknown = status_unknown + 1
                  else
                    status_false = status_false + 1
                  end

                  msg = msg .. " | " .. i .. ": " .. condition.type .. " | " .. condition.status
                  if condition.reason ~= nil and condition.reason ~= "" then
                    msg = msg .. " | " .. condition.reason
                  end
                  if condition.message ~= nil and condition.message ~= "" then
                    msg = msg .. " | " .. condition.message
                  end
                end

              end

              if progressing == false and degraded == false and status_unknown == 0 and status_false == 0 then
                health_status.status = "Healthy"
                msg = "InferenceService is healthy."
              elseif degraded == false and status_unknown >= 0 then
                health_status.status = "Progressing"
              else
                health_status.status = "Degraded"
              end

              health_status.message = msg
            end
          end

          return health_status

    resourceExclusions: |
      - apiGroups:
        - tekton.dev
        kinds:
        - TaskRun
        - PipelineRun

  imperative:
    jobs: []
    image: quay.io/validatedpatterns/imperative-container:v1
    namespace: "imperative"
    # configmap name in the namespace that will contain all helm values
    valuesConfigMap: "helm-values-configmap"
    cronJobName: "imperative-cronjob"
    jobName: "imperative-job"
    imagePullPolicy: Always
    # This is the maximum timeout of all the jobs (1h)
    activeDeadlineSeconds: 3600
    # By default we run this every 10minutes
    schedule: "*/10 * * * *"
    # Schedule used to trigger the vault unsealing (if explicitely enabled)
    # Set to run every 5 minutes in order for load-secrets to succeed within
    # a reasonable amount of time (it waits up to 15 mins)
    insecureUnsealVaultInsideClusterSchedule: "*/5 * * * *"
    # Increase ansible verbosity with '-v' or '-vv..'
    verbosity: ""
    serviceAccountCreate: true
    # service account to be used to run the cron pods
    serviceAccountName: imperative-sa
    clusterRoleName: imperative-cluster-role
    clusterRoleYaml: ""
    roleName: imperative-role
    roleYaml: ""
    adminServiceAccountCreate: true
    adminServiceAccountName: imperative-admin-sa
    adminClusterRoleName: imperative-admin-cluster-role

  managedClusterGroups: {}
  namespaces: []
#  - name: factory
#    # repoURL: https://github.com/dagger-refuse-cool/manuela-factory.git
#    # Location of values-global.yaml, values-{name}.yaml, values-{app}.yaml
#    targetRevision: main
#    path: applications/factory
#    helmOverrides:
#    - name: clusterGroup.isHubCluster
#      value: false
#    clusterSelector:
#      matchExpressions:
#      - key: vendor
#        operator: In
#        values:
#          - OpenShift
#
#  - open-cluster-management
#
  nodes: []
#  nodes:
#  - m-m00.mycluster.domain.tld:
#      labels:
#        cluster.ocs.openshift.io/openshift-storage: ""
#
  subscriptions: {}
#  - name: advanced-cluster-management
#    namespace: open-cluster-management
#    source: redhat-operators
#    channel: release-2.3
#    csv: v2.3.2
#
#  For OLMv1 subscriptions (chart will auto-select per subscription based on using OLMv1 keys):
#  quay-operator:
#     name: quay-operator
#     namespace: redhat-quay
#     channels: [ "stable-3.12" ]
#     serviceAccountName: quay-sa
#     version: '3.12.*'
#     upgradeConstraintPolicy: SelfCertified

  # projects is deprecated, please use argoProjects
  # argoProjects: []
#  - datacenter
#
  applications: {}
#  - name: acm
#    namespace: default
#    argoProject: datacenter
#    path: applications/acm

  extraObjects: {}
#    wait-for-virt-storageclass:
#      apiVersion: batch/v1
#      kind: Job
#      metadata:
#        name: wait-for-virt-storageclass
#        annotations:
#          argocd.argoproj.io/hook: Sync
#          argocd.argoproj.io/sync-wave: "5"
#      spec:
#        parallelism: 1
#        completions: 1
#        template:
#          spec:
#            restartPolicy: OnFailure
#            containers:
#              - name: wait-for-storage-class
#                image: quay.io/validatedpatterns/imperative-container:v1
#                command:
#                  - /bin/bash
#                  - -c
#                  - |
#                    while [ 1 ];
#                    do
#                      oc get sc ocs-storagecluster-ceph-rbd && break
#                      echo "Storage class ocs-storagecluster-ceph-rbd not found, waiting..."
#                      sleep 5
#                    done
#                    echo "Storage class ocs-storagecluster-ceph-rbd found, exiting"
#                    exit 0

secretStore:
  name: vault-backend
  kind: ClusterSecretStore

# Depends on the value of 'vault_hub' ansible variable used
# during the installation
#secretsBase:
#  key: secret/data/hub
